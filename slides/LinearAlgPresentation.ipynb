{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Algebra Review & Pytorch Introduction\n",
    "#### by: Michael Przystupa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### At the conclusion of this presentation, participants will be able to:\n",
    "- Distinguish scalars, vectors, matrices, and tensors\n",
    "- Recall several matrix mathematical operations\n",
    "- Recognize special types of matrices\n",
    "- Calculate basic linear algebra statements with pytorch\n",
    "- Recognize python statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "import torch \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 2.1: Scalars, Vectors, Matrices and Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scalar\n",
    "- A single number with an associated type \n",
    "  - e.g. real or natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' torch.LongTensor (Think of long as synonymous with integer)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Scalars in python\n",
    "a = 1 # integer\n",
    "b = 1. # float\n",
    "\n",
    "#Scalars in pytorch\n",
    "e = torch.tensor(a) # can pass variables as input\n",
    "f = torch.tensor(1.) # type of number will change tensor type\n",
    "\n",
    "display(\" {} (Think of long as synonymous with integer)\".format(e.type())) \n",
    "display(f.type())\n",
    "\n",
    "#Defining scalars as pytorch tensors is not requires for math operations:\n",
    "display(a * e)\n",
    "display (b + f.item()) #calling item gets the value stored in the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vector\n",
    "- An array of numbers\n",
    "- usually referenced as points in space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x: [1.0, 3.0, 4.0]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'y: tensor([ 2.,  3.,  8.])'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'z: tensor([ 2.,  3.,  8.])'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Vector e.g.\n",
    "x = [1., 3., 4.] #conceptually, could be viewed as a list\n",
    "y = torch.tensor([2.,3.,8.]) # Defining a vector in pytorch\n",
    "z = torch.tensor(y) # Can initialize a vector using a pre-defined tensor or list\n",
    "t = torch.from_numpy(np.array(x)) #in case you really like numpy\n",
    "display('x: ' + str(x))\n",
    "display('y: ' + str(y))\n",
    "display('z: ' + str(z))\n",
    "\n",
    "#adds stuff for accessing arrays (mention DL thoughts about indexing notation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Matrix: \n",
    "- a 2 dimensional array of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.],\n",
       "        [ 2.,  4.,  0.],\n",
       "        [ 3.,  5.,  7.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$A_{00} =tensor(1.)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$A_{ij} = tensor(7.)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$A_{i, :} = tensor([ 3.,  5.,  7.])$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$A_{:, j} =tensor([ 0.,  0.,  7.])$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = torch.tensor([\n",
    "    [1.,0, 0],\n",
    "    [2.,4.,0],\n",
    "    [3.,5.,7,]\n",
    "]) #as easy as declaring a python list of lists\n",
    "\n",
    "display(A)\n",
    "#Remember that python indexes start at 0!'\n",
    "display(Math('A_{00} =' + str(A[0,0]))) \n",
    "\n",
    "i, j = 2, 2 # this will be 3rd row and column\n",
    "display(Math('A_{ij} = ' + str(A[i,j])))\n",
    "display(Math('A_{i, :} = ' + str(A[i,:]))) #accessing a whole row\n",
    "display(Math('A_{:, j} =' + str(A[:,j]))) #accessing a whole column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tensor\n",
    "- A multi-dimensional array of numbers\n",
    "- Generally the term used when describing these data types\n",
    "- Each dimension is often referred to as an axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$B_{i, j, k} =tensor(1.00000e-02 *\n",
       "       5.0443)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$C_{i,:,:}=$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  2],\n",
       "        [ 2,  2]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$C_{:,j,:}=$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  2],\n",
       "        [ 3,  1],\n",
       "        [ 4,  5]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$C_{:,:,k}=$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  2],\n",
       "        [ 1,  1],\n",
       "        [ 5,  5]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code for using tensors\n",
    "A = torch.rand((3,3)) # we declare a matrix\n",
    "B = torch.stack([A,A, A], dim=2) #stack is a way to combine matrices into a tensor\n",
    "C = torch.tensor([\n",
    "    [ [2, 2],\n",
    "      [2, 2]],\n",
    "    [   [3, 1],\n",
    "        [3, 1]\n",
    "    ],[\n",
    "        [4, 5],\n",
    "        [4, 5]]\n",
    "]) #tensors can have more than 3 axis\n",
    "i,j,k = 0, 1, 1 \n",
    "display(Math('B_{i, j, k} =' + str(B[i,j,k])))\n",
    "display(Math('C_{i,:,:}='))\n",
    "display((C[i, :, :]))\n",
    "display(Math('C_{:,j,:}='))\n",
    "display((C[:, j, :]))\n",
    "display(Math('C_{:,:,k}=' ))\n",
    "display((C[:, :, k]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transpose \n",
    "- The mirror image of the the matrix across the main diagonal line\n",
    "- Achieved by swapping each row $i^{th}$ position with associated column $j^{th}$ position:\n",
    "  - $A_{ij} \\rightarrow A_{ji}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.],\n",
       "        [ 2.,  4.,  0.],\n",
       "        [ 3.,  5.,  7.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.],\n",
       "        [ 0.,  4.,  5.],\n",
       "        [ 0.,  0.,  7.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1],\n",
       "        [ 2],\n",
       "        [ 3]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Tranpose examples\n",
    "#Given some Matrix:\n",
    "A = torch.tensor([\n",
    "    [1.,0, 0],\n",
    "    [2.,4.,0],\n",
    "    [3.,5.,7,]\n",
    "])\n",
    "# You can tranpose it with pytorch like this:\n",
    "A_t = A.t() #also torch.transpose(A,0, 1) would work\n",
    "display(A)\n",
    "display(A_t)\n",
    "\n",
    "# Vectors:\n",
    "x = torch.tensor([[1,2,3]]) #note here I have a list in a list even though it's a vector\n",
    "display(x.size()) #You might do this to explicitly identify this as a row or column vector\n",
    "display(x) #this will be viewed as a row vector or a 1 x 3 matrix\n",
    "display(x.t()) #the transpose makes it a column vector or a 3 x 1 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Matrix Addition\n",
    "\n",
    "- If 2 matrices have the same shape we can add their them element-wise:\n",
    "  - $A_{ij} + B_{ij} = C_{ij},\\forall i,j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  3],\n",
       "        [ 4,  4]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d82e70a97ddf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This will work because they are the same size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this will fail because C is twice as big\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([\n",
    "    [2,2],\n",
    "    [3,3]\n",
    "])\n",
    "B = torch.tensor([\n",
    "    [1,1],\n",
    "    [1,1]\n",
    "])\n",
    "C = torch.cat([B, B]) # another way of combining several matrices\n",
    "\n",
    "display(A + B) # This will work because they are the same size\n",
    "display(A + C) # this will fail because C is twice as big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scalar operations on a matrix\n",
    "- Multiplying a matrix by a scalar changes each element\n",
    "  - This applies for addition and other operations as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  3.,  3.],\n",
       "        [ 3.,  3.,  3.],\n",
       "        [ 3.,  3.,  3.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.,  8.,  8.],\n",
       "        [ 8.,  8.,  8.],\n",
       "        [ 8.,  8.,  8.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = 3.0\n",
    "B = torch.ones((3,3)) #a special matrix where all entries are ones\n",
    "display(a * B)\n",
    "\n",
    "c = 5.0 #we define a scalar here, note again it is not a tensor\n",
    "D = a * B + c #an affine function operation\n",
    "E = (D - c) / a # here we see that dividing still works as well\n",
    "display(D)\n",
    "display(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Broadcasting\n",
    "- Deep learning allows adding vectors directly matrices:\n",
    "  - It's like copying a vector to be a matrix of the same size\n",
    "  - Dimensions of vector must be equal to rows or columns of matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  3.,  2.,  4.],\n",
       "        [ 1.,  3.,  2.,  4.],\n",
       "        [ 1.,  3.,  2.,  4.],\n",
       "        [ 1.,  3.,  2.,  4.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  3.,  2.,  4.],\n",
       "        [ 1.,  3.,  2.,  4.],\n",
       "        [ 1.,  3.,  2.,  4.],\n",
       "        [ 1.,  3.,  2.,  4.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.,  1.],\n",
       "        [ 3.,  3.,  3.,  3.],\n",
       "        [ 2.,  2.,  2.,  2.],\n",
       "        [ 4.,  4.,  4.,  4.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = torch.zeros((4,4))\n",
    "b = torch.tensor([1., 3., 2., 4.])\n",
    "c = b.unsqueeze(0) #an operation that adds a dimension to an existing tensor\n",
    "d = c.t()\n",
    "\n",
    "display(A + b) \n",
    "display(A + c)\n",
    "display(A + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 2.2 Multiplying Matrices and Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dot Product:\n",
    "- For vectors a and b with same size , the dot product combines them into a scalar:\n",
    "\n",
    "\\begin{equation*}\n",
    " a \\cdot b = \\sum_{i=1}^n a_i * b_i\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(26)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = [3, 4, 5]\n",
    "b = [1, 2, 3]\n",
    "def dotprod(x, y):\n",
    "    assert(len(x) == len(y)) #makes sure both are same length\n",
    "    #acc = 0.0\n",
    "    #for i in range(0, len(x)):\n",
    "        # acc = acc + x[i] * y[i]\n",
    "    #return acc\n",
    "    #below is called a list comprehension and allows us to write the above compactly\n",
    "    return sum([x[i] * y[i] for i in range(0, len(x))]) \n",
    "display(dotprod(a,b))\n",
    "display(torch.dot(torch.tensor(a), torch.tensor(b)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Matrix Product:\n",
    "- Like applying dot products for multiple entries at once\n",
    "- Requires Matrix A to have same number of columns as Matrix B has rows:\n",
    "    - $A \\epsilon R^{mxn}$ and $B \\epsilon R^{nxp}$\n",
    "    - produces new matrix $C \\epsilon R^{mxp}$ \n",
    "- Here is the formula for each entry:\n",
    "\\begin{equation*}\n",
    "  C_{ij} = \\sum_{i=1}^n A_{i,k}B_{k, j}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 14,  23,  32],\n",
       "        [ 17,  28,  39]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = torch.tensor([\n",
    "    [1,2,3],\n",
    "    [2,3,4]\n",
    "])\n",
    "B = torch.tensor([\n",
    "    [4,5],\n",
    "    [5,6]\n",
    "])\n",
    "\n",
    "#optional exercise for at home: implement the matrix product yourself!\n",
    "display(A.size())\n",
    "display(B.size())\n",
    "display(torch.mm(B, A)) #why is B before A?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Element-wise (Hadamard) Product: \n",
    "- For $A \\epsilon R^{mxn}$ and $B \\epsilon R^{mxn}$ we multiply each of the elements directly, so for each entry of $C = A \\odot B$:\n",
    "\n",
    "\\begin{equation*}\n",
    "C_{ij} = A_{ij}*B_{ij}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2,   6],\n",
       "        [ 12,  20]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = torch.tensor([\n",
    "    [1,2],\n",
    "    [3,4]\n",
    "])\n",
    "B = torch.tensor([\n",
    "    [2, 3],\n",
    "    [4, 5]\n",
    "])\n",
    "\n",
    "display(A * B) #it's like multiplying scalars, but matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Matrix Properties:\n",
    "- Lets just see the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#The matrices we'll use for each property\n",
    "A = torch.tensor([\n",
    "    [1,2,3],\n",
    "    [2,3,4]\n",
    "]) #2 x 3 matrix\n",
    "B = torch.tensor([\n",
    "    [4,5],\n",
    "    [5,6]\n",
    "]) # 2 x2 matrix\n",
    "C = B + 1 # another 2x2 matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Distributive Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$(B + C)A = BA + CA$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 31,  51,  71],\n",
       "        [ 37,  61,  85]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 31,  51,  71],\n",
       "        [ 37,  61,  85]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math('(B + C)A = BA + CA'))\n",
    "display(torch.mm( B + C, A))\n",
    "display(torch.mm(B, A) + torch.mm(C, A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Associative Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$(BC)A = B(CA)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 168,  277,  386],\n",
       "        [ 205,  338,  471]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 168,  277,  386],\n",
       "        [ 205,  338,  471]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math('(BC)A = B(CA)'))\n",
    "display(torch.mm(torch.mm(B, C), A))\n",
    "display(torch.mm(B, torch.mm(C, A) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Commutative Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AB != BA'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [2 x 3], m2: [2 x 2] at /opt/conda/conda-bld/pytorch_1524580938250/work/aten/src/TH/generic/THTensorMath.c:2033",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-9f7662dcb790>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Matrices ARE NOT commutative!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AB != BA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [2 x 3], m2: [2 x 2] at /opt/conda/conda-bld/pytorch_1524580938250/work/aten/src/TH/generic/THTensorMath.c:2033"
     ]
    }
   ],
   "source": [
    "#Matrices ARE NOT commutative!\n",
    "display('AB != BA')\n",
    "torch.mm(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Linear Systems of Equations\n",
    "\n",
    "- Probably will never be used in class, but lets see them anyways!\n",
    "- For known matrix $A \\epsilon R^{mxn}$, $ b \\epsilon R^{m}$, and $x \\epsilon R^{m}$ of unknowns:\n",
    "\n",
    "\\begin{equation*}\n",
    " Ax = b\n",
    "\\end{equation*}\n",
    "- The objective is to then find x\n",
    "- pytorch has some solvers to do this:\n",
    "  - https://pytorch.org/docs/stable/torch.html?highlight=linear%20solver#torch.gesv\n",
    "    - link to one solver, although several ways to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 2.3: Identity and Inverse Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Identity Matrix\n",
    "- A matrix that does not change any vector when that matrix and the vector are multiplied.\n",
    "  - Often denoted with I:\n",
    "\\begin{equation*}\n",
    " AI = A\n",
    "\\end{equation*}  \n",
    "- This is a matrix with 1's on the main diagonal and 0's every where else\n",
    "  - Generally a square matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7159,  0.8091,  0.1056],\n",
       "        [ 0.4336,  0.9223,  0.8569],\n",
       "        [ 0.6757,  0.4213,  0.8328]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7159,  0.8091,  0.1056],\n",
       "        [ 0.4336,  0.9223,  0.8569],\n",
       "        [ 0.6757,  0.4213,  0.8328]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Identity matrix examples\n",
    "A = torch.rand((3,3)) # a matrix of randomly generated numbers\n",
    "display(torch.eye(3))\n",
    "display(torch.eye(4))\n",
    "display(A)\n",
    "display(torch.mm(A, torch.eye(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Matrix Inverse\n",
    "- For a matrix A, the inverse is such that:\n",
    "\\begin{equation*}\n",
    "A^{-1} A = I_{n}\n",
    "\\end{equation*}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.],\n",
       "        [ 2.,  3.,  0.],\n",
       "        [ 4.,  5.,  6.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.0000,  0.0000],\n",
       "        [-0.6667,  0.3333, -0.0000],\n",
       "        [-0.1111, -0.2778,  0.1667]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  1.0000, -0.0000],\n",
       "        [ 0.0000, -0.0000,  1.0000]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = torch.tensor([\n",
    "    [1., 0. ,0.],\n",
    "    [2., 3., 0.],\n",
    "    [4., 5., 6.]\n",
    "])\n",
    "\n",
    "display(A)\n",
    "display(A.inverse())\n",
    "display(torch.mm(A.inverse(), A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Solving Linear equations with Inverses\n",
    "- You can use matrix inversion to solve linear systems of equation. However, the matrix A must be square and not singular:\n",
    "\n",
    "\\begin{equation*}\n",
    "A^{-1}Ax = x =  A^{-1}b\n",
    "\\end{equation*}\n",
    "\n",
    "- What does any of that mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 2.4 Linear Dependence and Span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## $A^{-1} \\exists \\iff Ax = b$ has a unique solution $\\forall b \\epsilon R^{m}$\n",
    "- $Ax = b$ has exactly 3 options for solutions:\n",
    "  - $x$ is unique solution\n",
    "  - $x$ has $\\infty$ solutions\n",
    "    - for 2 solutions $a$ & $b$ there is $z = \\alpha a + (1-\\alpha) b$ other solutions, $\\alpha \\epsilon R$ \n",
    "  - $x$ has no solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear Combination\n",
    "- For a set of vectors $v^{(1)},..., v^{(n)}$ and scalars $c_{1},...c_{n}$ create new vector $v'$:\n",
    "\\begin{equation*}\n",
    "v' = \\sum_{i}^{n} c_{i}v^{(i)}\n",
    "\\end{equation*}\n",
    "- A solution to $Ax = b$ is a linear combination of the vectors in A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.6000,  5.0000,  3.6000])\n"
     ]
    }
   ],
   "source": [
    "v1 = torch.tensor([1.,2., 3.])\n",
    "v2 = torch.tensor([2.,7., 4.])\n",
    "c1 = .4\n",
    "c2 = .6\n",
    "vhat = c1*v1 + c2*v2\n",
    "print(vhat) # your goto for displaying variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Span\n",
    "- All points obtainable by linear combination of a set of vectors $v^{(1)}...v^{(n)}$\n",
    "- Column Space (Range): the span of for a matrix A when solving $Ax=b$, i.e. all the vectors b that are combinations of the columns of A\n",
    "  - if b not in the span of A, then there is no solution\n",
    "  - To have a solution having at least as many columns as rows is probably important\n",
    "    - If number of columns < number of rows then there are values we can't represent\n",
    "    - Is it enough though?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1a04ef2fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#A visual to hopefully get idea of span across\n",
    "#Using the above facts here's a visual\n",
    "A = torch.tensor([\n",
    "    [.5, 1],\n",
    "    [1, -.5]\n",
    "])\n",
    "#plt.quiver([0, 0], [0, 0], [.5, 1], [1, -.5 ], angles='xy', scale_units='xy', scale=1)\n",
    "plt.quiver([0, 0], [0, 0], A[:,0], A[:,1], angles='xy', scale_units='xy', scale=1)\n",
    "plt.plot(1,2, 'g^') #an arbitrary choice for solution b\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear Independence\n",
    "- For a set of vectors $v^{(1)}...v^{(n)}$, no vector in the set is a combination of other vectors in the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = torch.tensor([\n",
    "    [1,-2, 1],\n",
    "    [0, 2,-2],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "#Generally is obvious if you can get your matrix in the form above\n",
    "# the above  matrix also called a \"square matrix\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear Dependence\n",
    "- For a set of vectors $v^{(1)}...v^{(n)}$ , $\\exists$ a vector which is a combination of other vectors in the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = torch.tensor([\n",
    "    [1,-2, 1, 4],\n",
    "    [0, 2,-2, 2],\n",
    "    [0, 0, 1, 3]    \n",
    "]) #is now dependent as 4th column is combination of other columns\n",
    "B = torch.tensor([\n",
    "    [2, 1],\n",
    "    [2, 1]\n",
    "]) #also dependent because identical, this is referred to as a singular matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Putting it all together for when $A^{-1}$ exists\n",
    "\n",
    "- We need at least as many linear independent columns as rows:\n",
    "  - If not, then there are points in space we cannot represent ( b's with no solution)\n",
    "- We need there to be only as many columns as rows:\n",
    "  - If not, then some columns could be redundant (b's with infinite solutions)\n",
    "- From the above 2 this means we need a square, nonsingular matrix to have an inverse:\n",
    "  - Note that if the square matrix is singular, there could be a solution but we cannot invert\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 2.5 Norms\n",
    "- Functions that measure the size of vectors\n",
    "- They map values to non-negative values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## $L^{P}$ Norm:\n",
    "- One class of norms\n",
    "\\begin{equation*}\n",
    "||x||_{p} = ( \\sum_{i} |x_{i}|^p)^{1 / p}\n",
    "\\end{equation*}\n",
    "  - $p\\epsilon R,p \\geq 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Let's look at some classic choices of norms\n",
    "A = torch.tensor([\n",
    "    [3.],\n",
    "    [2.],\n",
    "    [1.]\n",
    "]) #our friendly neighborhood torch tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## L2 (Euclidean) Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$L2: ||x||_{2} = \\sqrt{\\sum_{i}x_{i}^2}$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 3.7417])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.7417)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$||x||_{2}^{2} = {\\sum_{i}x_{i}^2}$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 14.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(14.0000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Euclidean Norm, also known as the L2 norm:\n",
    "display(Math('L2: ||x||_{2} = \\sqrt{\\sum_{i}x_{i}^2}'))\n",
    "display(sum(A[i]**2 for i in range(0, len(A))) **.5)\n",
    "display(torch.norm(A, p=2)) # how to do it in pytorch\n",
    "\n",
    "#Often you'll see the Squared L2 norm, which is really just a dot product:\n",
    "display(Math('||x||_{2}^{2} = {\\sum_{i}x_{i}^2}'))\n",
    "display(torch.mm(A.t(), A)) # ez pz\n",
    "display(torch.norm(A, p=2) **2.)\n",
    "#Note: People like them because the gradient is nice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## L1 Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$L1: ||x||_{1} = \\sum_{i}|x_{i}|$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 6.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# L1 norm is a popular one which take the absolute value of numbers\n",
    "display(Math('L1: ||x||_{1} = \\sum_{i}|x_{i}|'))\n",
    "display(sum(abs(A[i]) for i in range(0, len(A))))\n",
    "display(torch.norm(A, p=1)) # how to do it in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other Popular Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$L^{\\infty} = max_{i} |x_{i}|$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 3.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$L_{0}: ||x||_{0} = \\sum_{i}x_{i}^{0}$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$||A||_{F} = \\sqrt{\\sum_{ij}A_{ij}^{2}}$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the Max norm\n",
    "display(Math('L^{\\infty} = max_{i} |x_{i}|'))\n",
    "display(max(A)) #don't even need pytorch for this one...\n",
    "display(torch.max(A)) #but there is a function\n",
    "\n",
    "#L0 norm which isn't a norm because it violates a norm properties\n",
    "display(Math('L_{0}: ||x||_{0} = \\sum_{i}x_{i}^{0}'))\n",
    "display(sum(1 for i in range(0, len(A)) if A[i] is not 0))\n",
    "display(torch.norm(A, p=0)) # how to do it in pytorch\n",
    "\n",
    "# Frobenius norm: Think of it as a generalized L2 norm for matrices\n",
    "display(Math(\"||A||_{F} = \\sqrt{\\sum_{ij}A_{ij}^{2}}\"))\n",
    "#option take home problem: write a function to calculate Frobenius norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Properties that define a Norm:\n",
    "\n",
    "- $f(x) = 0 \\Rightarrow x = 0$\n",
    "- $f(x + y) \\leq f(x) + f(y)$\n",
    "- $\\forall \\alpha \\epsilon R, f(\\alpha x) = \\alpha f(x)$\n",
    "  - this is why the L0 norm is not a norm, the number of  nonzero entries don't change with scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Another view of dot products: \n",
    "- Norms are useful because we can rewrite the dot product using them:\n",
    "\\begin{equation*}\n",
    "x^{T} y = ||x||_{2} ||y||_{2} cos(\\theta)\n",
    "\\end{equation*}\n",
    "  - $\\theta$ is the angle between the two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(20.0000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.tensor([1.,2.,3.])\n",
    "y = torch.tensor([2.,3.,4.])\n",
    "theta = torch.tensor(0.121850677) #theta can be found with algebra\n",
    "\n",
    "display(x.dot(y))\n",
    "display(torch.norm(x, p=2) * torch.norm(y,p=2) * torch.cos(theta) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 2.6 Special kinds of Matrices and Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Diagonal Matrix\n",
    "- A matrix that has nonzero entries along its main diagonal. \n",
    "- Easy to invert:\n",
    "  - Take the inverse of each element along main diagonal\n",
    "- Easy to multiply by:\n",
    "  - Scale each entry of a vector by the corresponding entry on the diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  0.,  0.],\n",
       "        [ 0.,  2.,  0.],\n",
       "        [ 0.,  0.,  1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3333,  0.0000, -0.0000],\n",
       "        [ 0.0000,  0.5000, -0.0000],\n",
       "        [ 0.0000,  0.0000,  1.0000]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "I = torch.eye(3) # the identiy matrix is an example of diagonal matrices\n",
    "A = torch.tensor([\n",
    "    [3., 0., 0.],\n",
    "    [0., 2., 0.],\n",
    "    [0., 0., 1.]\n",
    "])\n",
    "display(A)\n",
    "display(A.inverse())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Symmetric Matrix\n",
    "- A matrix that is equal to its own inverse \n",
    "- Often entries generated by a function that is independent of the argument order\n",
    "  - e.g. A matrix of distances between measurements because distance is symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  0,  3],\n",
       "        [ 0,  1,  0],\n",
       "        [ 3,  0,  1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  0,  3],\n",
       "        [ 0,  1,  0],\n",
       "        [ 3,  0,  1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "I = torch.eye(3)\n",
    "display(I)\n",
    "display(I.t())\n",
    "\n",
    "A = torch.tensor([\n",
    "    [1, 0, 3],\n",
    "    [0, 1, 0],\n",
    "    [3, 0, 1]\n",
    "])\n",
    "display(A)\n",
    "display(A.t())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unit Vector\n",
    "- A vector who has unit length 1\n",
    "\\begin{equation*}\n",
    "||x||_{2} = 1\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = torch.tensor([ 12. / 13., -3./ 13., -4./ 13.]) #borrowed from the internet\n",
    "display(r.dot(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Orthogonality and Orthonormality\n",
    "- 2 vectors are **orthogonal** if for vectors x & y we have: $x\\cdot y = 0$\n",
    "\n",
    "- 2 vectors are **orthonormal** if they are both unit vectors and orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#An example of just orthogonal vectors\n",
    "x = torch.tensor([1, 0, 3])\n",
    "y = torch.tensor([0, 1, 0])\n",
    "display(x.dot(y))\n",
    "\n",
    "#A super simple example of orthonormal vectors\n",
    "x = torch.tensor([1, 0])\n",
    "y = torch.tensor([0, 1])\n",
    "display(x.dot(x))\n",
    "display(y.dot(y))\n",
    "display(y.dot(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Orthogonal Matrix:\n",
    "- A square matrix whos rows are mutually orthornormal and columns are mutually orthonormal\n",
    "- With math symbols: $A^{T}A = AA^{T} = I$\n",
    "  - this implies: $A^{-1} = A^{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00, -3.3114e-09,  3.3114e-09],\n",
       "        [-3.3114e-09,  1.0000e+00,  6.6227e-09],\n",
       "        [ 3.3114e-09,  6.6227e-09,  1.0000e+00]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00, -3.3114e-08, -2.3180e-08],\n",
       "        [ 2.6491e-08,  1.0000e+00, -2.6491e-08],\n",
       "        [ 6.6227e-09,  3.3114e-09,  1.0000e+00]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6667,  0.3333,  0.6667],\n",
       "        [-0.6667,  0.6667,  0.3333],\n",
       "        [ 0.3333,  0.6667, -0.6667]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6667,  0.3333,  0.6667],\n",
       "        [-0.6667,  0.6667,  0.3333],\n",
       "        [ 0.3333,  0.6667, -0.6667]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A simple example\n",
    "B = (1./3.) * torch.tensor([\n",
    "    [2.,-2., 1.],\n",
    "    [1., 2., 2.],\n",
    "    [2., 1.,-2.]\n",
    "])\n",
    "\n",
    "display(torch.mm(B, B.t())) #will produce close to identity (should be identity)\n",
    "display(torch.mm(B.inverse(), B)) #also close to identity (should be identity)\n",
    "display(B.inverse())\n",
    "display(B.t())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The End "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Meet and Greet:\n",
    "- Introduce yourself (name, faculty, favorite ice cream)\n",
    "- Answer this question: What are your research goals for this course? i.e. Why do YOU want to learn deep learning and it's utility to NLP?\n",
    "- 2 options:\n",
    "  1. Separate into groups of 2 - 3\n",
    "  2. Go around class and each person says it to everybody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
